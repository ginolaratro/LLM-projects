{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrooIE+etGvnGAB5kbJTEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ginolaratro/LLM-projects/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2HKjdJm3w5x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359f8d7f"
      },
      "source": [
        "### Explanation of the BPETokenizer Class and Results\n",
        "\n",
        "We've implemented a custom `BPETokenizer` class that encapsulates the BPE logic. Here's a breakdown of its key components and how it operates based on the executed code:\n",
        "\n",
        "1.  **`__init__(self, text, num_merges)`**: The constructor initializes the tokenizer with the training text and the desired number of merges. It then calls the `train()` method.\n",
        "\n",
        "2.  **`_get_initial_tokens()`**: This helper method prepares the input text by splitting it into words and converting each word into a list of characters, appending a special `</w>` token to mark word boundaries. This character-level representation forms the initial vocabulary.\n",
        "\n",
        "3.  **`_get_stats(word_tokens)`**: This function iterates through the current list of word tokens (which can be individual characters or merged subword units) and counts the frequency of all adjacent pairs.\n",
        "\n",
        "4.  **`_merge_pair(word_tokens, pair, new_token)`**: This function takes the current word tokens, the most frequent `pair` identified, and a `new_token` (which is the concatenation of the pair). It then replaces all occurrences of that `pair` in the `word_tokens` list with the `new_token`.\n",
        "\n",
        "5.  **`train()`**: This is the core training method. It iteratively performs `num_merges`:\n",
        "    *   It calls `_get_stats()` to find the most frequent pair.\n",
        "    *   It creates a `new_token` from this pair.\n",
        "    *   It updates the `merges` dictionary to record this operation (`(pair): new_token`).\n",
        "    *   It calls `_merge_pair()` to apply the merge across all current word tokens.\n",
        "    *   The `new_token` is added to the `vocabulary`.\n",
        "    \n",
        "    The output `Learned merges` shows the sequence of pairs that were merged and their resulting tokens, e.g., `('a', 't'): 'at'`.\n",
        "\n",
        "6.  **`tokenize(text_to_tokenize)`**: This method takes a new sentence and applies all the learned merges in the order they were discovered during training. It starts by breaking the new text into character-level tokens (with `</w>`) and then iteratively replaces character pairs with learned subword units until no more merges can be applied. The `Encoded tokens` output demonstrates this process, showing how words like \"cat\" and \"hat\" are represented by their learned subword tokens `cat</w>` and `hat</w>` respectively.\n",
        "\n",
        "7.  **`decode(tokens)`**: This simple method reverses the tokenization by joining the tokens back into a string and replacing the `</w>` tokens with spaces to reconstruct the original text. The `Decoded text` output confirms that the tokenization and decoding process is reversible and accurate.\n",
        "\n",
        "This implementation provides a clear illustration of how BPE incrementally builds a subword vocabulary and uses it to tokenize text, handling both common words and out-of-vocabulary terms efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31f7d859"
      },
      "source": [
        "### What is Byte Pair Encoding (BPE)?\n",
        "\n",
        "Byte Pair Encoding (BPE) is a data compression technique that is also widely used in natural language processing (NLP) for tokenization, especially in subword tokenization. It works by iteratively merging the most frequent pair of adjacent characters or character sequences into a new, single token.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1.  **Initialize Vocabulary:** Start with a vocabulary of individual characters present in the training text.\n",
        "2.  **Count Pairs:** Identify all adjacent pairs of tokens in the training data and count their frequencies.\n",
        "3.  **Merge Most Frequent Pair:** Replace all occurrences of the most frequent pair with a new, combined token.\n",
        "4.  **Repeat:** Go back to step 2 and repeat the process for a fixed number of merge operations or until the vocabulary reaches a desired size.\n",
        "\n",
        "This process creates a vocabulary of common subword units, which helps in handling out-of-vocabulary words (by breaking them down into known subwords) and reducing the overall vocabulary size compared to character-level tokenization, while being more flexible than word-level tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a59a1b45",
        "outputId": "a938fb19-1469-4ebc-a4bc-f6e662a1012f"
      },
      "source": [
        "# Step 1: Prepare the input text\n",
        "text = \"\"\"I have a cat. My cat has a hat. I like my cat with a hat.\"\"\"\n",
        "\n",
        "# For simplicity, we'll start with character-level tokens and add a special token for word boundaries\n",
        "# This helps in distinguishing 'cat' from 'catch' for example.\n",
        "# We'll also convert to a list of lists of characters for processing.\n",
        "\n",
        "words = text.split()\n",
        "initial_tokens = []\n",
        "for word in words:\n",
        "    # Add a special end-of-word token '</w>' to each word\n",
        "    initial_tokens.append(list(word) + ['</w>'])\n",
        "\n",
        "print(\"Initial tokens (character-level with </w>):\")\n",
        "for tokens in initial_tokens:\n",
        "    print(tokens)\n",
        "\n",
        "vocabulary = set(char for word_tokens in initial_tokens for char in word_tokens)\n",
        "print(f\"\\nInitial vocabulary: {sorted(list(vocabulary))}\")\n",
        "\n",
        "# Helper function to convert a list of tokens back to a string for display\n",
        "def tokens_to_string(tokens_list):\n",
        "    return [''.join(tokens).replace('</w>', '_') for tokens in tokens_list]\n",
        "\n",
        "print(f\"\\nInitial processed text representation: {tokens_to_string(initial_tokens)}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial tokens (character-level with </w>):\n",
            "['I', '</w>']\n",
            "['h', 'a', 'v', 'e', '</w>']\n",
            "['a', '</w>']\n",
            "['c', 'a', 't', '.', '</w>']\n",
            "['M', 'y', '</w>']\n",
            "['c', 'a', 't', '</w>']\n",
            "['h', 'a', 's', '</w>']\n",
            "['a', '</w>']\n",
            "['h', 'a', 't', '.', '</w>']\n",
            "['I', '</w>']\n",
            "['l', 'i', 'k', 'e', '</w>']\n",
            "['m', 'y', '</w>']\n",
            "['c', 'a', 't', '</w>']\n",
            "['w', 'i', 't', 'h', '</w>']\n",
            "['a', '</w>']\n",
            "['h', 'a', 't', '.', '</w>']\n",
            "\n",
            "Initial vocabulary: ['.', '</w>', 'I', 'M', 'a', 'c', 'e', 'h', 'i', 'k', 'l', 'm', 's', 't', 'v', 'w', 'y']\n",
            "\n",
            "Initial processed text representation: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e91f0dd",
        "outputId": "9b79616d-c889-4a07-ab31-93b4486924c7"
      },
      "source": [
        "# Step 2: Implement the core BPE functions\n",
        "\n",
        "# Function to count frequencies of adjacent pairs\n",
        "def get_stats(word_tokens):\n",
        "    pairs = {}\n",
        "    for word in word_tokens:\n",
        "        for i in range(len(word) - 1):\n",
        "            pair = (word[i], word[i+1])\n",
        "            pairs[pair] = pairs.get(pair, 0) + 1\n",
        "    return pairs\n",
        "\n",
        "# Function to merge the most frequent pair\n",
        "def merge_pair(word_tokens, pair, new_token):\n",
        "    merged_word_tokens = []\n",
        "    for word in word_tokens:\n",
        "        new_word = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            if i < len(word) - 1 and (word[i], word[i+1]) == pair:\n",
        "                new_word.append(new_token)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        merged_word_tokens.append(new_word)\n",
        "    return merged_word_tokens\n",
        "\n",
        "\n",
        "# Let's run a few merges\n",
        "current_word_tokens = initial_tokens.copy()\n",
        "merges = {}\n",
        "num_merges = 10 # Number of merge operations to perform\n",
        "\n",
        "print(\"\\n--- Performing BPE Merges ---\")\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(current_word_tokens)\n",
        "    if not pairs:\n",
        "        print(\"No more pairs to merge. Stopping.\")\n",
        "        break\n",
        "\n",
        "    # Find the most frequent pair\n",
        "    most_frequent_pair = max(pairs, key=pairs.get)\n",
        "    new_token = ''.join(most_frequent_pair)\n",
        "    merges[most_frequent_pair] = new_token\n",
        "\n",
        "    print(f\"\\nMerge {i+1}: Merging {most_frequent_pair} into '{new_token}' (count: {pairs[most_frequent_pair]})\")\n",
        "    current_word_tokens = merge_pair(current_word_tokens, most_frequent_pair, new_token)\n",
        "    print(f\"Current tokens: {tokens_to_string(current_word_tokens)}\")\n",
        "\n",
        "print(\"\\n--- BPE Merges Complete ---\")\n",
        "print(f\"Final merges learned: {merges}\")\n",
        "\n",
        "# Update vocabulary with new tokens\n",
        "final_vocabulary = set(token for word_tokens in current_word_tokens for token in word_tokens)\n",
        "print(f\"Final vocabulary size: {len(final_vocabulary)}\")\n",
        "print(f\"Final vocabulary: {sorted(list(final_vocabulary))}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Performing BPE Merges ---\n",
            "\n",
            "Merge 1: Merging ('a', 't') into 'at' (count: 5)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 2: Merging ('a', '</w>') into 'a</w>' (count: 3)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 3: Merging ('c', 'at') into 'cat' (count: 3)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 4: Merging ('.', '</w>') into '.</w>' (count: 3)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 5: Merging ('I', '</w>') into 'I</w>' (count: 2)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 6: Merging ('h', 'a') into 'ha' (count: 2)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 7: Merging ('e', '</w>') into 'e</w>' (count: 2)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 8: Merging ('y', '</w>') into 'y</w>' (count: 2)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 9: Merging ('cat', '</w>') into 'cat</w>' (count: 2)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "Merge 10: Merging ('h', 'at') into 'hat' (count: 2)\n",
            "Current tokens: ['I_', 'have_', 'a_', 'cat._', 'My_', 'cat_', 'has_', 'a_', 'hat._', 'I_', 'like_', 'my_', 'cat_', 'with_', 'a_', 'hat._']\n",
            "\n",
            "--- BPE Merges Complete ---\n",
            "Final merges learned: {('a', 't'): 'at', ('a', '</w>'): 'a</w>', ('c', 'at'): 'cat', ('.', '</w>'): '.</w>', ('I', '</w>'): 'I</w>', ('h', 'a'): 'ha', ('e', '</w>'): 'e</w>', ('y', '</w>'): 'y</w>', ('cat', '</w>'): 'cat</w>', ('h', 'at'): 'hat'}\n",
            "Final vocabulary size: 20\n",
            "Final vocabulary: ['.</w>', '</w>', 'I</w>', 'M', 'a</w>', 'cat', 'cat</w>', 'e</w>', 'h', 'ha', 'hat', 'i', 'k', 'l', 'm', 's', 't', 'v', 'w', 'y</w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b21d4771",
        "outputId": "a843c3e8-463f-44d5-eef7-bff274efeb03"
      },
      "source": [
        "# Step 3: Create a BPE tokenizer class for encoding and decoding\n",
        "\n",
        "class BPETokenizer:\n",
        "    def __init__(self, text, num_merges):\n",
        "        self.text = text\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = {}\n",
        "        self.vocabulary = set()\n",
        "        self.byte_to_char = {i: chr(i) for i in range(256)} # For real BPE, using bytes\n",
        "        self.char_to_byte = {chr(i): i for i in range(256)}\n",
        "        self.train()\n",
        "\n",
        "    def _get_initial_tokens(self):\n",
        "        words = self.text.split()\n",
        "        initial_tokens_list = []\n",
        "        for word in words:\n",
        "            initial_tokens_list.append(list(word) + ['</w>'])\n",
        "        return initial_tokens_list\n",
        "\n",
        "    def _get_stats(self, word_tokens):\n",
        "        pairs = {}\n",
        "        for word in word_tokens:\n",
        "            for i in range(len(word) - 1):\n",
        "                pair = (word[i], word[i+1])\n",
        "                pairs[pair] = pairs.get(pair, 0) + 1\n",
        "        return pairs\n",
        "\n",
        "    def _merge_pair(self, word_tokens, pair, new_token):\n",
        "        merged_word_tokens = []\n",
        "        for word in word_tokens:\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                if i < len(word) - 1 and (word[i], word[i+1]) == pair:\n",
        "                    new_word.append(new_token)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            merged_word_tokens.append(new_word)\n",
        "        return merged_word_tokens\n",
        "\n",
        "    def train(self):\n",
        "        current_word_tokens = self._get_initial_tokens()\n",
        "        self.vocabulary = set(token for word_tokens in current_word_tokens for token in word_tokens)\n",
        "\n",
        "        for _ in range(self.num_merges):\n",
        "            pairs = self._get_stats(current_word_tokens)\n",
        "            if not pairs:\n",
        "                break\n",
        "            most_frequent_pair = max(pairs, key=pairs.get)\n",
        "            new_token = ''.join(most_frequent_pair)\n",
        "            self.merges[most_frequent_pair] = new_token\n",
        "            current_word_tokens = self._merge_pair(current_word_tokens, most_frequent_pair, new_token)\n",
        "            self.vocabulary.add(new_token)\n",
        "\n",
        "        # Reverse merges for encoding process: from merged token to its components\n",
        "        self.reversed_merges = {v: k for k, v in self.merges.items()}\n",
        "\n",
        "    def tokenize(self, text_to_tokenize):\n",
        "        # Start with character-level tokens for the new text\n",
        "        words_to_tokenize = text_to_tokenize.split()\n",
        "        encoded_tokens_list = []\n",
        "\n",
        "        for word in words_to_tokenize:\n",
        "            word_chars = list(word) + ['</w>']\n",
        "            # Apply merges in the order they were learned\n",
        "            for pair, new_token in self.merges.items():\n",
        "                i = 0\n",
        "                while i < len(word_chars) - 1:\n",
        "                    if (word_chars[i], word_chars[i+1]) == pair:\n",
        "                        word_chars[i:i+2] = [new_token]\n",
        "                    else:\n",
        "                        i += 1\n",
        "            encoded_tokens_list.extend(word_chars)\n",
        "        return encoded_tokens_list\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join(tokens)\n",
        "        text = text.replace('</w>', ' ')\n",
        "        return text.strip()\n",
        "\n",
        "\n",
        "# Instantiate and train the tokenizer\n",
        "bpe_tokenizer = BPETokenizer(text, num_merges=10)\n",
        "\n",
        "print(f\"\\nLearned merges: {bpe_tokenizer.merges}\")\n",
        "print(f\"Final vocabulary: {sorted(list(bpe_tokenizer.vocabulary))}\")\n",
        "\n",
        "# Test encoding and decoding\n",
        "input_sentence = \"My cat has a big hat.\"\n",
        "encoded_tokens = bpe_tokenizer.tokenize(input_sentence)\n",
        "decoded_text = bpe_tokenizer.decode(encoded_tokens)\n",
        "\n",
        "print(f\"\\nOriginal sentence: '{input_sentence}'\")\n",
        "print(f\"Encoded tokens: {encoded_tokens}\")\n",
        "print(f\"Decoded text: '{decoded_text}'\")\n",
        "\n",
        "# Example with a word not seen in training\n",
        "new_sentence = \"The big dog. The small cat.\"\n",
        "encoded_new_tokens = bpe_tokenizer.tokenize(new_sentence)\n",
        "decoded_new_text = bpe_tokenizer.decode(encoded_new_tokens)\n",
        "\n",
        "print(f\"\\nNew sentence: '{new_sentence}'\")\n",
        "print(f\"Encoded new tokens: {encoded_new_tokens}\")\n",
        "print(f\"Decoded new text: '{decoded_new_text}'\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learned merges: {('a', 't'): 'at', ('a', '</w>'): 'a</w>', ('c', 'at'): 'cat', ('.', '</w>'): '.</w>', ('I', '</w>'): 'I</w>', ('h', 'a'): 'ha', ('e', '</w>'): 'e</w>', ('y', '</w>'): 'y</w>', ('cat', '</w>'): 'cat</w>', ('h', 'at'): 'hat'}\n",
            "Final vocabulary: ['.', '.</w>', '</w>', 'I', 'I</w>', 'M', 'a', 'a</w>', 'at', 'c', 'cat', 'cat</w>', 'e', 'e</w>', 'h', 'ha', 'hat', 'i', 'k', 'l', 'm', 's', 't', 'v', 'w', 'y', 'y</w>']\n",
            "\n",
            "Original sentence: 'My cat has a big hat.'\n",
            "Encoded tokens: ['M', 'y</w>', 'cat</w>', 'ha', 's', '</w>', 'a</w>', 'b', 'i', 'g', '</w>', 'hat', '.</w>']\n",
            "Decoded text: 'My cat has a big hat.'\n",
            "\n",
            "New sentence: 'The big dog. The small cat.'\n",
            "Encoded new tokens: ['T', 'h', 'e</w>', 'b', 'i', 'g', '</w>', 'd', 'o', 'g', '.</w>', 'T', 'h', 'e</w>', 's', 'm', 'a', 'l', 'l', '</w>', 'cat', '.</w>']\n",
            "Decoded new text: 'The big dog. The small cat.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yIEhsweNkSWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11cfdeba",
        "outputId": "6219b95e-3434-439b-faf3-ee9c5d8fe978"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Example input: 'logits' from a neural network output\n",
        "# Let's say we have 3 classes and a batch size of 1\n",
        "logits = tf.constant([2.0, 1.0, 0.1])\n",
        "print(f\"Input logits: {logits.numpy()}\")\n",
        "\n",
        "# Apply the softmax function\n",
        "probabilities = tf.nn.softmax(logits)\n",
        "\n",
        "print(f\"Softmax probabilities: {probabilities.numpy()}\")\n",
        "print(f\"Sum of probabilities: {tf.reduce_sum(probabilities).numpy()}\")\n",
        "\n",
        "# Example with a batch of inputs\n",
        "batch_logits = tf.constant([\n",
        "    [2.0, 1.0, 0.1],  # Example 1\n",
        "    [0.5, 2.5, 1.5]   # Example 2\n",
        "])\n",
        "print(f\"\\nInput batch logits:\\n{batch_logits.numpy()}\")\n",
        "\n",
        "batch_probabilities = tf.nn.softmax(batch_logits)\n",
        "\n",
        "print(f\"Softmax batch probabilities:\\n{batch_probabilities.numpy()}\")\n",
        "print(f\"Sum of probabilities for each example in batch: {tf.reduce_sum(batch_probabilities, axis=1).numpy()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input logits: [2.  1.  0.1]\n",
            "Softmax probabilities: [0.6590012  0.24243298 0.09856589]\n",
            "Sum of probabilities: 1.0000001192092896\n",
            "\n",
            "Input batch logits:\n",
            "[[2.  1.  0.1]\n",
            " [0.5 2.5 1.5]]\n",
            "Softmax batch probabilities:\n",
            "[[0.6590012  0.24243298 0.09856589]\n",
            " [0.09003057 0.6652409  0.24472845]]\n",
            "Sum of probabilities for each example in batch: [1.0000001 0.9999999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e121d760"
      },
      "source": [
        "### Explanation of TensorFlow Softmax Implementation\n",
        "\n",
        "In the Python code above, we demonstrated how to use TensorFlow's built-in `tf.nn.softmax` function.\n",
        "\n",
        "1.  **`import tensorflow as tf`**: We start by importing the TensorFlow library.\n",
        "\n",
        "2.  **`logits = tf.constant([2.0, 1.0, 0.1])`**: We define a `tf.constant` tensor named `logits`. This represents the raw, unnormalized scores for three different classes. For example, if a model predicts that an input has a score of 2.0 for class A, 1.0 for class B, and 0.1 for class C.\n",
        "\n",
        "3.  **`probabilities = tf.nn.softmax(logits)`**: This is where the magic happens! We pass our `logits` tensor to `tf.nn.softmax()`. TensorFlow efficiently computes the softmax function as per the formula discussed earlier:\n",
        "    *   It calculates $e^{z_i}$ for each element.\n",
        "    *   It then normalizes these exponential values by dividing each by the sum of all exponential values.\n",
        "\n",
        "4.  **`print(f\"Softmax probabilities: {probabilities.numpy()}\")`**: The output `probabilities` tensor contains values between 0 and 1, representing the probability distribution over the classes. The class with the highest logit will have the highest probability.\n",
        "\n",
        "5.  **`print(f\"Sum of probabilities: {tf.reduce_sum(probabilities).numpy()}\")`**: This line confirms that the sum of the probabilities for a single input always equals 1, which is a fundamental property of probability distributions.\n",
        "\n",
        "6.  **Batch Example**: We also show how `tf.nn.softmax` handles a batch of inputs (`batch_logits`). When given a 2D tensor, `tf.nn.softmax` applies the softmax function independently to each row (representing a different example in the batch). `axis=1` in `tf.reduce_sum` ensures we sum across the classes for each example to verify that each row's probabilities sum to 1."
      ]
    }
  ]
}