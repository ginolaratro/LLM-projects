{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjA8mkys6SqU8qvksProez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ginolaratro/LLM-projects/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSK6hkt3mgFk",
        "outputId": "a438d42c-52aa-44bf-db42-38c710e0933e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.2763 - loss: 1.1086\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3429 - loss: 1.1022 \n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3209 - loss: 1.0997\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.2863 - loss: 1.1041\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3205 - loss: 1.0995\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3320 - loss: 1.0980\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3535 - loss: 1.0996\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3378 - loss: 1.0965\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3513 - loss: 1.0944\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3938 - loss: 1.0956\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f79292d32c0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Fake dataset\n",
        "# 100 samples, 4 features each\n",
        "X = np.random.rand(100, 4)\n",
        "\n",
        "# 3 classes (0, 1, 2)\n",
        "y = np.random.randint(0, 3, size=(100,))\n",
        "\n",
        "# Build model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(3)  # <- 3 outputs = 3 logits\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train\n",
        "model.fit(X, y, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-y6zJL1oKHb",
        "outputId": "7eb23919-0a6b-49e8-83da-251b688f8330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.59455109, 0.09297815, 0.78743409, 0.08490913],\n",
              "       [0.82434318, 0.83552866, 0.3682319 , 0.77578502],\n",
              "       [0.47267006, 0.84834805, 0.55696681, 0.98466905],\n",
              "       [0.77185967, 0.89500357, 0.04069778, 0.16641197],\n",
              "       [0.77560737, 0.19604355, 0.82140162, 0.71919577],\n",
              "       [0.16634876, 0.94514934, 0.73647979, 0.12058142],\n",
              "       [0.48569275, 0.53951813, 0.40495994, 0.44572868],\n",
              "       [0.1682396 , 0.08077451, 0.86209302, 0.16174118],\n",
              "       [0.69208107, 0.6043585 , 0.78623047, 0.36185601],\n",
              "       [0.54486464, 0.47842145, 0.55151662, 0.70251362],\n",
              "       [0.58999854, 0.31693625, 0.63276763, 0.88390825],\n",
              "       [0.88331353, 0.88395552, 0.67680744, 0.74079422],\n",
              "       [0.58756076, 0.58073074, 0.67742973, 0.06449461],\n",
              "       [0.45954617, 0.6626138 , 0.07750123, 0.86144959],\n",
              "       [0.99794706, 0.19059556, 0.5885372 , 0.99939481],\n",
              "       [0.59284585, 0.2556106 , 0.45029435, 0.69505588],\n",
              "       [0.32663461, 0.49227349, 0.45669221, 0.57123094],\n",
              "       [0.82603208, 0.97828315, 0.22348034, 0.29334232],\n",
              "       [0.64999705, 0.24956973, 0.40879617, 0.78949597],\n",
              "       [0.30652695, 0.04289537, 0.6016984 , 0.58963657],\n",
              "       [0.2474397 , 0.19665215, 0.08336657, 0.63526364],\n",
              "       [0.26512008, 0.94542367, 0.11318334, 0.85298724],\n",
              "       [0.37012373, 0.31550344, 0.36449498, 0.4554523 ],\n",
              "       [0.78621743, 0.35295877, 0.61093287, 0.54906995],\n",
              "       [0.1459782 , 0.75252699, 0.88714062, 0.08880538],\n",
              "       [0.10019476, 0.44508069, 0.91119153, 0.58287268],\n",
              "       [0.3790013 , 0.8230092 , 0.16275387, 0.65107258],\n",
              "       [0.50124442, 0.46589369, 0.63085572, 0.52092596],\n",
              "       [0.98490424, 0.96026188, 0.45648836, 0.31623643],\n",
              "       [0.79949418, 0.76724265, 0.06693119, 0.4044506 ],\n",
              "       [0.39423373, 0.10804222, 0.12264406, 0.15061646],\n",
              "       [0.13059108, 0.94593822, 0.19839471, 0.61691962],\n",
              "       [0.04195401, 0.43150669, 0.964746  , 0.99395351],\n",
              "       [0.1713978 , 0.05784079, 0.54364568, 0.18248379],\n",
              "       [0.60147277, 0.1758971 , 0.80384736, 0.33744116],\n",
              "       [0.78692704, 0.41493294, 0.79415536, 0.67021542],\n",
              "       [0.23206913, 0.31154309, 0.46286978, 0.25825378],\n",
              "       [0.55472633, 0.15434229, 0.2220511 , 0.40176144],\n",
              "       [0.48886761, 0.3658994 , 0.64412057, 0.23772814],\n",
              "       [0.78789668, 0.28098921, 0.28599702, 0.22308614],\n",
              "       [0.63964062, 0.89898792, 0.86569042, 0.03344186],\n",
              "       [0.36127976, 0.56950672, 0.81032122, 0.92812024],\n",
              "       [0.49576309, 0.74594639, 0.35574263, 0.3803746 ],\n",
              "       [0.66336801, 0.17002015, 0.12756406, 0.61133709],\n",
              "       [0.24245398, 0.65412117, 0.71506823, 0.46383812],\n",
              "       [0.61902039, 0.29603516, 0.12342351, 0.50629342],\n",
              "       [0.57866902, 0.13903395, 0.95746146, 0.29384832],\n",
              "       [0.12713498, 0.45417156, 0.9094053 , 0.03622609],\n",
              "       [0.68229902, 0.98774963, 0.98235763, 0.7874118 ],\n",
              "       [0.71784051, 0.55857809, 0.06309306, 0.02598589],\n",
              "       [0.06724821, 0.50176255, 0.30068636, 0.70657912],\n",
              "       [0.19612401, 0.87824978, 0.02884857, 0.84122682],\n",
              "       [0.80729458, 0.62059076, 0.26936614, 0.47978695],\n",
              "       [0.45835651, 0.76034596, 0.90338867, 0.36096175],\n",
              "       [0.22110815, 0.32672324, 0.25479328, 0.34899262],\n",
              "       [0.90210401, 0.37961966, 0.35284406, 0.83262669],\n",
              "       [0.44798434, 0.01776807, 0.3323971 , 0.00392981],\n",
              "       [0.06218828, 0.20432839, 0.74795228, 0.02960444],\n",
              "       [0.07567958, 0.78524651, 0.45227424, 0.3957615 ],\n",
              "       [0.10176528, 0.44991566, 0.44000348, 0.57172395],\n",
              "       [0.82262747, 0.73682058, 0.94554498, 0.48031533],\n",
              "       [0.3071631 , 0.74213917, 0.75842441, 0.33599024],\n",
              "       [0.57222506, 0.53934396, 0.04005455, 0.82095469],\n",
              "       [0.94548823, 0.448394  , 0.78410572, 0.68534252],\n",
              "       [0.41524727, 0.64768036, 0.96298153, 0.07257066],\n",
              "       [0.65721661, 0.46748452, 0.80469173, 0.79472697],\n",
              "       [0.62317041, 0.31505217, 0.90704773, 0.96503356],\n",
              "       [0.91927846, 0.03241634, 0.51445564, 0.17768254],\n",
              "       [0.89143029, 0.81458896, 0.76900679, 0.21804227],\n",
              "       [0.67204692, 0.85813683, 0.29726207, 0.39868591],\n",
              "       [0.21506574, 0.9807846 , 0.24033891, 0.29118954],\n",
              "       [0.40302892, 0.59606518, 0.08200565, 0.4219443 ],\n",
              "       [0.32254015, 0.08701867, 0.12497964, 0.30529507],\n",
              "       [0.53263181, 0.04117261, 0.70836151, 0.07733214],\n",
              "       [0.12308261, 0.40366461, 0.93468409, 0.37568718],\n",
              "       [0.59729424, 0.89123111, 0.0741797 , 0.82750792],\n",
              "       [0.15527823, 0.61834262, 0.86236826, 0.06685687],\n",
              "       [0.00454726, 0.47508364, 0.38747857, 0.00645017],\n",
              "       [0.15457748, 0.38293638, 0.66338073, 0.42222729],\n",
              "       [0.84282152, 0.06399414, 0.851208  , 0.51590308],\n",
              "       [0.98287914, 0.53844992, 0.08555165, 0.91016434],\n",
              "       [0.18527906, 0.83405281, 0.9621762 , 0.88553541],\n",
              "       [0.67960474, 0.06350557, 0.94638639, 0.32929039],\n",
              "       [0.8537651 , 0.76226094, 0.90947256, 0.87128473],\n",
              "       [0.21651051, 0.14545507, 0.43518233, 0.706084  ],\n",
              "       [0.64097392, 0.25304193, 0.71401425, 0.74503522],\n",
              "       [0.8606965 , 0.14893915, 0.9969126 , 0.28805691],\n",
              "       [0.54109455, 0.49028612, 0.25534735, 0.36114986],\n",
              "       [0.48106569, 0.10626743, 0.08895334, 0.72379248],\n",
              "       [0.08606773, 0.60749596, 0.94589969, 0.50768633],\n",
              "       [0.60472719, 0.22895984, 0.88072133, 0.24821321],\n",
              "       [0.43892729, 0.31868163, 0.16980802, 0.33393703],\n",
              "       [0.789048  , 0.19705211, 0.87071939, 0.96512826],\n",
              "       [0.98539568, 0.90502766, 0.08613741, 0.94845526],\n",
              "       [0.96555775, 0.71951618, 0.57412458, 0.23454409],\n",
              "       [0.45282201, 0.98553473, 0.51028506, 0.74338393],\n",
              "       [0.16017092, 0.17068466, 0.83607831, 0.41877694],\n",
              "       [0.43904007, 0.58138395, 0.26976563, 0.43866048],\n",
              "       [0.91884858, 0.82008818, 0.2950805 , 0.62198619],\n",
              "       [0.89873214, 0.50235865, 0.81535433, 0.76390656]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load real handwritten digit data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf-TCTfIoaVW",
        "outputId": "cf194621-c0b7-47ef-f97b-15156654bc89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),  # turn image into 784 numbers\n",
        "    tf.keras.layers.Dense(128, activation='relu'),  # learn patterns\n",
        "    tf.keras.layers.Dense(10)  # 10 logits (for digits 0–9)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqXLllx6oflR",
        "outputId": "3060acdd-e64b-4e67-c27e-87407b10ad01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1078 - loss: 2.3018\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.1122 - loss: 2.3013\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.1133 - loss: 2.3014\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1140 - loss: 2.3011\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.1119 - loss: 2.3013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f7924625c10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iemNvokrY-I",
        "outputId": "0f5f6d4f-1d99-4ad4-b563-c620e25595fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1160 - loss: 2.3013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.301264762878418, 0.11349999904632568]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = X_test[0:1]\n",
        "\n",
        "logits = model(sample)\n",
        "print(\"Logits:\", logits.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi5jbXaLrecH",
        "outputId": "395ec50a-c31b-4deb-f845-203c086cd99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: [[ 0.00197627  0.11738122 -0.03177003  0.0131141  -0.030847   -0.08435042\n",
            "  -0.00325851  0.03813067 -0.0354943  -0.02360712]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = tf.nn.softmax(logits)\n",
        "\n",
        "print(\"Probabilities:\", probs.numpy())\n",
        "print(\"Predicted digit:\", np.argmax(probs))\n",
        "print(\"Actual digit:\", y_test[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WpXoaxkrlod",
        "outputId": "3c6af8a6-ab24-44db-d2da-4151d3cc2757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilities: [[0.10045296 0.11274116 0.09711961 0.10157804 0.09720929 0.09214496\n",
            "  0.09992848 0.10415123 0.09675858 0.09791563]]\n",
            "Predicted digit: 1\n",
            "Actual digit: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logits are the raw, unnormalized output scores (ranging from \\(-\\infty \\) to \\(+\\infty \\)) from the final layer of a neural network before an activation function (like sigmoid or softmax) is applied. They represent the model's initial prediction confidence for classification tasks, where higher values indicate a higher likelihood for a class"
      ],
      "metadata": {
        "id": "zCTVJrc6sQTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will implement Cross-Entropy Loss.\n",
        "This will tell us how accurate our results are\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rOurO_jlHJG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np # Added for consistency, though not strictly needed for this specific loss calculation here.\n",
        "\n",
        "# The previous execution failed because 'y_test' and 'logits' were not defined in the current kernel session.\n",
        "# To demonstrate the cross-entropy loss calculation, we will re-initialize these variables\n",
        "# using the values that were successfully computed and displayed in previous cells.\n",
        "\n",
        "# Re-defining logits with the value obtained from `mi5jbXaLrecH`\n",
        "logits = tf.constant([[ 0.00197627,  0.11738122, -0.03177003,  0.0131141,  -0.030847,   -0.08435042,\n",
        "                      -0.00325851,  0.03813067, -0.0354943,  -0.02360712]], dtype=tf.float32)\n",
        "\n",
        "# Re-defining y_test[0] with the actual digit value obtained from `9WpXoaxkrlod`\n",
        "actual_digit = 7\n",
        "y_test_single_sample = tf.constant([actual_digit], dtype=tf.int32) # Needs to be a tensor with batch dimension\n",
        "\n",
        "# Calculate Sparse Categorical Crossentropy Loss for the sample\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Calculate the loss for the sample\n",
        "sample_loss = loss_object(y_test_single_sample, logits)\n",
        "\n",
        "print(f\"Logits for the sample: {logits.numpy().flatten()}\")\n",
        "print(f\"Actual digit for the sample: {actual_digit}\")\n",
        "print(f\"Calculated Sparse Categorical Crossentropy Loss: {sample_loss.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uLMO_lsGr8b",
        "outputId": "23258c59-be78-4be4-ff5e-966eba656be0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for the sample: [ 0.00197627  0.11738122 -0.03177003  0.0131141  -0.030847   -0.08435042\n",
            " -0.00325851  0.03813067 -0.0354943  -0.02360712]\n",
            "Actual digit for the sample: 7\n",
            "Calculated Sparse Categorical Crossentropy Loss: 2.261911153793335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "757c60d1"
      },
      "source": [
        "# Task\n",
        "Implement Positional Encoding for a Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45b2cb80"
      },
      "source": [
        "## Understand Transformer Architecture\n",
        "\n",
        "### Subtask:\n",
        "Start by understanding the key components of a transformer model, including self-attention, multi-head attention, positional encoding, and the encoder-decoder structure. This will inform the implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd28b454"
      },
      "source": [
        "### Understanding the Transformer Architecture\n",
        "\n",
        "The Transformer model, introduced in the paper \"Attention Is All You Need,\" revolutionized sequence-to-sequence tasks, particularly in natural language processing. Unlike recurrent neural networks (RNNs) and convolutional neural networks (CNNs), it relies entirely on attention mechanisms to draw global dependencies between input and output.\n",
        "\n",
        "#### Key Components:\n",
        "\n",
        "1.  **Self-Attention (Scaled Dot-Product Attention):**\n",
        "    *   This mechanism allows the model to weigh the importance of different words in the input sequence when encoding a single word. It calculates a 'score' for each word relative to every other word, indicating how much attention to pay to other parts of the sequence.\n",
        "    *   It uses three learnable matrices: Query (Q), Key (K), and Value (V).\n",
        "    *   The attention score is calculated as: `Attention(Q, K, V) = softmax(Q * K_transpose / sqrt(d_k)) * V`.\n",
        "\n",
        "2.  **Multi-Head Attention:**\n",
        "    *   Instead of performing a single attention function, multi-head attention runs `h` different attention mechanisms in parallel. Each 'head' learns different aspects of the relationships between words.\n",
        "    *   The outputs from these `h` attention heads are concatenated and then linearly transformed into the desired dimension.\n",
        "    *   This allows the model to jointly attend to information from different representation subspaces at different positions.\n",
        "\n",
        "3.  **Positional Encoding:**\n",
        "    *   Since the Transformer architecture does not inherently process sequences in order (due to the lack of recurrence or convolution), positional encoding is added to the input embeddings to inject information about the relative or absolute position of tokens in the sequence.\n",
        "    *   These encodings are typically fixed (not learned) sinusoidal functions of different frequencies.\n",
        "\n",
        "4.  **Feed-Forward Networks (FFN):**\n",
        "    *   Each position in the Transformer's encoder and decoder contains a fully connected feed-forward network, which is applied identically and independently to each position.\n",
        "    *   This typically consists of two linear transformations with a ReLU activation in between: `FFN(x) = max(0, x * W1 + b1) * W2 + b2`.\n",
        "\n",
        "5.  **Residual Connections & Layer Normalization:**\n",
        "    *   **Residual Connections:** Each sub-layer (self-attention, feed-forward) in the encoder and decoder is surrounded by a residual connection (`x + Sublayer(x)`), which helps in training deeper networks by allowing gradients to flow through bypass paths.\n",
        "    *   **Layer Normalization:** After the residual connection, layer normalization is applied. Unlike batch normalization, layer normalization normalizes the inputs across the features for each sample independently, rather than across a batch.\n",
        "\n",
        "6.  **Encoder-Decoder Structure:**\n",
        "    *   **Encoder:** Composed of a stack of `N` identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The encoder processes the input sequence and produces a sequence of continuous representations.\n",
        "    *   **Decoder:** Also composed of a stack of `N` identical layers. Each layer has three sub-layers: a masked multi-head self-attention mechanism (to prevent attending to future positions during training), a multi-head attention layer that attends to the output of the encoder, and a position-wise fully connected feed-forward network. The decoder generates the output sequence one token at a time, taking the encoder's output and the previously generated tokens as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fe05ae5"
      },
      "source": [
        "## Implement Positional Encoding\n",
        "\n",
        "### Subtask:\n",
        "Implement the positional encoding mechanism that adds information about the position of tokens in the sequence, as transformers do not inherently process sequences in order.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "277c77cd"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import the necessary libraries, `tensorflow` and `numpy`, as specified in the instructions to prepare for defining the positional encoding components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c825bd49",
        "outputId": "a3dd8611-24fe-4be5-d64b-a38652baf8b8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"TensorFlow and NumPy imported successfully.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow and NumPy imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9890de65"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, the next step is to define the `get_angles` function which calculates the arguments for the sine and cosine functions, as required for positional encoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9301d274",
        "outputId": "f72b06f8-56be-4b64-b634-d97e2bdc8f0e"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "print(\"get_angles function defined.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_angles function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8de9165"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the instructions, the next step is to create the `PositionalEncoding` class, which will generate the positional encodings using the previously defined `get_angles` function and the sine/cosine formulas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bb6e445",
        "outputId": "8c0faa39-40e2-4034-e9c1-b491550c673f"
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
        "                                     np.arange(d_model)[np.newaxis, :],\n",
        "                                     d_model)\n",
        "\n",
        "        # apply sin to even indices in the array; 2i\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "        # apply cos to odd indices in the array; 2i + 1\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "print(\"PositionalEncoding class defined.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PositionalEncoding class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cafd2a25"
      },
      "source": [
        "**Reasoning**:\n",
        "To complete the subtask, the next step is to test the `PositionalEncoding` class by creating an instance with specific `position` (max_sequence_length) and `d_model` values, and then printing the shape of the generated positional encodings to verify the implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffea4b1d",
        "outputId": "cc021e1d-5c95-4744-e204-f84fbc1307cf"
      },
      "source": [
        "max_sequence_length = 50\n",
        "d_model = 512\n",
        "\n",
        "# Create an instance of PositionalEncoding\n",
        "pos_encoding_layer = PositionalEncoding(max_sequence_length, d_model)\n",
        "\n",
        "# Access the generated positional encodings\n",
        "# The pos_encoding attribute already has the batch dimension (1, max_sequence_length, d_model)\n",
        "positional_encodings = pos_encoding_layer.pos_encoding\n",
        "\n",
        "print(f\"Shape of generated positional encodings: {positional_encodings.shape}\")\n",
        "\n",
        "# You can also test the call method by providing dummy input\n",
        "dummy_inputs = tf.zeros((1, 10, d_model), dtype=tf.float32) # Batch size 1, sequence length 10\n",
        "output_with_pe = pos_encoding_layer(dummy_inputs)\n",
        "print(f\"Shape of output after adding positional encodings (dummy input): {output_with_pe.shape}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of generated positional encodings: (1, 50, 512)\n",
            "Shape of output after adding positional encodings (dummy input): (1, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdce423e"
      },
      "source": [
        "## Prepare Data for Transformer\n",
        "\n",
        "### Subtask:\n",
        "Prepare dummy data including tokenization, numericalization, padding, and batching to simulate a sequence-to-sequence task for demonstrating Transformer components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4f3e91f"
      },
      "source": [
        "**Reasoning**:\n",
        "To prepare dummy data for the Transformer, the first step is to define simple integer-based dummy source and target vocabularies, which will be used for numericalization, and then create dummy input and target sequences based on these vocabularies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a399cb3",
        "outputId": "b277b767-7075-4c8e-c104-537d49aad732"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define dummy source and target vocabularies\n",
        "source_vocab = {\n",
        "    0: '<pad>', 1: '<start>', 2: '<end>',\n",
        "    3: 'hello', 4: 'world', 5: 'how',\n",
        "    6: 'are', 7: 'you', 8: 'i',\n",
        "    9: 'am', 10: 'fine'\n",
        "}\n",
        "target_vocab = {\n",
        "    0: '<pad>', 1: '<start>', 2: '<end>',\n",
        "    3: 'bonjour', 4: 'monde', 5: 'comment',\n",
        "    6: 'allez', 7: 'vous', 8: 'je',\n",
        "    9: 'vais', 10: 'bien'\n",
        "}\n",
        "\n",
        "# Create reverse vocabularies for easy lookup (e.g., for numericalization)\n",
        "# In a real scenario, you'd typically have a tokenizer that maps text to IDs\n",
        "source_id_to_word = {v: k for k, v in source_vocab.items()}\n",
        "target_id_to_word = {v: k for k, v in target_vocab.items()}\n",
        "\n",
        "# 2. Create dummy input and target sequences (numericalized)\n",
        "# Sequences are represented as lists of integers (token IDs)\n",
        "# Adding <start> and <end> tokens for target sequences usually\n",
        "# Input: \"hello world how are you\"\n",
        "# Target: \"bonjour monde comment allez vous\"\n",
        "input_sequences_raw = [\n",
        "    [source_id_to_word['hello'], source_id_to_word['world'], source_id_to_word['how'], source_id_to_word['are'], source_id_to_word['you']],\n",
        "    [source_id_to_word['i'], source_id_to_word['am'], source_id_to_word['fine']],\n",
        "    [source_id_to_word['how'], source_id_to_word['are'], source_id_to_word['you'], source_id_to_word['hello']]\n",
        "]\n",
        "target_sequences_raw = [\n",
        "    [target_id_to_word['<start>'], target_id_to_word['bonjour'], target_id_to_word['monde'], target_id_to_word['comment'], target_id_to_word['allez'], target_id_to_word['vous'], target_id_to_word['<end>']],\n",
        "    [target_id_to_word['<start>'], target_id_to_word['je'], target_id_to_word['vais'], target_id_to_word['bien'], target_id_to_word['<end>']],\n",
        "    [target_id_to_word['<start>'], target_id_to_word['comment'], target_id_to_word['allez'], target_id_to_word['vous'], target_id_to_word['bonjour'], target_id_to_word['<end>']]\n",
        "]\n",
        "\n",
        "print(\"Dummy vocabularies and raw sequences created.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy vocabularies and raw sequences created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "980f896a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dummy vocabularies and raw sequences are defined, the next logical step is to determine a maximum sequence length, pad these raw sequences to that length using the padding token, and then group them into TensorFlow tensor batches for further processing, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60504953",
        "outputId": "4744c75c-3e43-49f8-c804-b25459bb86ac"
      },
      "source": [
        "PAD_ID = 0\n",
        "\n",
        "# Determine max sequence length for padding (can be adjusted based on needs)\n",
        "max_input_seq_len = max(len(seq) for seq in input_sequences_raw)\n",
        "max_target_seq_len = max(len(seq) for seq in target_sequences_raw)\n",
        "\n",
        "# 3. Pad the dummy input and target sequences\n",
        "# Using tf.keras.preprocessing.sequence.pad_sequences for convenience\n",
        "padded_input_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    input_sequences_raw, maxlen=max_input_seq_len, padding='post', value=PAD_ID\n",
        ")\n",
        "padded_target_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    target_sequences_raw, maxlen=max_target_seq_len, padding='post', value=PAD_ID\n",
        ")\n",
        "\n",
        "# 4. Group the padded sequences into dummy batches (convert to tf.Tensor)\n",
        "# For simplicity, we'll use all sequences as a single batch\n",
        "input_batch = tf.constant(padded_input_sequences, dtype=tf.int32)\n",
        "target_batch = tf.constant(padded_target_sequences, dtype=tf.int32)\n",
        "\n",
        "# 5. Print the shapes and a sample to verify\n",
        "print(f\"Shape of input_batch: {input_batch.shape}\")\n",
        "print(f\"Shape of target_batch: {target_batch.shape}\")\n",
        "print(f\"\\nSample input sequence (numerical): {input_batch[0].numpy()}\")\n",
        "print(f\"Sample target sequence (numerical): {target_batch[0].numpy()}\")\n",
        "\n",
        "# Optionally, decode the sample to words for better understanding\n",
        "def decode_sequence(sequence, vocab):\n",
        "    return ' '.join([vocab[idx] for idx in sequence if idx != PAD_ID])\n",
        "\n",
        "print(f\"\\nSample input sequence (decoded): {decode_sequence(input_batch[0].numpy(), source_vocab)}\")\n",
        "print(f\"Sample target sequence (decoded): {decode_sequence(target_batch[0].numpy(), target_vocab)}\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input_batch: (3, 5)\n",
            "Shape of target_batch: (3, 7)\n",
            "\n",
            "Sample input sequence (numerical): [3 4 5 6 7]\n",
            "Sample target sequence (numerical): [1 3 4 5 6 7 2]\n",
            "\n",
            "Sample input sequence (decoded): hello world how are you\n",
            "Sample target sequence (decoded): <start> bonjour monde comment allez vous <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce0c7311"
      },
      "source": [
        "## Build Multi-Head Attention Mechanism\n",
        "\n",
        "### Subtask:\n",
        "Develop the multi-head attention layer, which allows the model to jointly attend to information from different representation subspaces at different positions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8641f6a1"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the first step in developing the `MultiHeadAttention` layer is to define the `MultiHeadAttention` class inheriting from `tf.keras.layers.Layer`, and within its `__init__` method, initialize `d_model`, `num_heads`, calculate `depth`, and set up the necessary `tf.keras.layers.Dense` layers for queries, keys, values, and the final output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ba7ee7",
        "outputId": "067b5c80-c5e7-4564-d7d0-45efcd56a8da"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        print(f\"MultiHeadAttention initialized with d_model={d_model}, num_heads={num_heads}, depth={self.depth}\")\n",
        "\n",
        "print(\"MultiHeadAttention class definition started with __init__.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttention class definition started with __init__.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32aa82d2"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to implement the `split_heads` method within the `MultiHeadAttention` class, which will reshape the input tensor to prepare it for multi-head attention.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "528b43d1",
        "outputId": "ee22e164-31ca-4150-9f9b-c366bf2186a3"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        print(f\"MultiHeadAttention initialized with d_model={d_model}, num_heads={num_heads}, depth={self.depth}\")\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "print(\"MultiHeadAttention class updated with split_heads method.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttention class updated with split_heads method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8717f78"
      },
      "source": [
        "**Reasoning**:\n",
        "As per the instructions, the next step is to implement the `scaled_dot_product_attention` function, which performs the core attention calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c65cf122",
        "outputId": "6d37eb0d-af60-47f5-f7dd-4599cadd23ac"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "print(\"scaled_dot_product_attention function defined.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaled_dot_product_attention function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fac27f6"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to implement the `call` method within the `MultiHeadAttention` class, which will orchestrate the dense transformations, splitting heads, performing scaled dot-product attention, and concatenating results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1156e0b7",
        "outputId": "c714f1b2-f8e9-4885-b505-895f2c6e9ed3"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "print(\"MultiHeadAttention class updated with call method.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttention class updated with call method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4092381b"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the final step for this subtask is to test the `MultiHeadAttention` layer by creating an instance with arbitrary `d_model` and `num_heads`, and then passing dummy input tensors to its `call` method to verify the shapes of the output and attention weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd1ac36a",
        "outputId": "010009df-2135-4a7b-d4fb-9d120254735b"
      },
      "source": [
        "num_heads = 8\n",
        "d_model = 512\n",
        "batch_size = 64\n",
        "seq_len = 100 # Example sequence length\n",
        "\n",
        "# Create an instance of MultiHeadAttention\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# Create dummy input tensors for query, key, and value\n",
        "# In self-attention, Q, K, V are typically the same input sequence\n",
        "dummy_input = tf.random.uniform((batch_size, seq_len, d_model))\n",
        "\n",
        "# Assuming no mask for this initial test\n",
        "output, attn_weights = mha(v=dummy_input, k=dummy_input, q=dummy_input, mask=None)\n",
        "\n",
        "print(f\"MultiHeadAttention output shape: {output.shape}\") # Expected: (batch_size, seq_len, d_model)\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\") # Expected: (batch_size, num_heads, seq_len, seq_len)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttention output shape: (64, 100, 512)\n",
            "Attention weights shape: (64, 8, 100, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d83cba3f"
      },
      "source": [
        "## Construct Transformer Encoder Layer\n",
        "\n",
        "### Subtask:\n",
        "Implement a single transformer encoder layer, consisting of a multi-head self-attention mechanism and a position-wise feed-forward network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e634d9db"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the first step to construct the Transformer Encoder Layer is to define the `point_wise_feed_forward_network` helper function, which will be used within the `EncoderLayer` to perform position-wise transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43beda69",
        "outputId": "ca8666d4-8685-44f5-99d0-6679ee7ba133"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])\n",
        "\n",
        "print(\"point_wise_feed_forward_network function defined.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "point_wise_feed_forward_network function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65549ce3"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to define the `EncoderLayer` class inheriting from `tf.keras.layers.Layer` and implement its `__init__` method, initializing `d_model`, `num_heads`, `MultiHeadAttention`, `point_wise_feed_forward_network`, `LayerNormalization` and `Dropout` layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27a08547",
        "outputId": "e9cc2ce4-c202-4ed6-ee22-d52ee4ae97c9"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "        print(f\"EncoderLayer initialized with d_model={d_model}, num_heads={num_heads}, dff={dff}, dropout_rate={rate}\")\n",
        "\n",
        "print(\"EncoderLayer class definition started with __init__.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncoderLayer class definition started with __init__.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79aae58f"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to implement the `call` method within the `EncoderLayer` class, which will orchestrate the multi-head self-attention, feed-forward network, residual connections, layer normalization, and dropout.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41228dd8",
        "outputId": "9e31dbfc-994d-4056-bafb-5eaff66f1af3"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        # Multi-head attention (self-attention)\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "\n",
        "print(\"EncoderLayer class updated with call method.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncoderLayer class updated with call method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6400615f"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the final step for this subtask is to test the `EncoderLayer` by creating an instance with arbitrary parameters and passing dummy input tensors to its `call` method to verify the output shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71bca930",
        "outputId": "24d97c42-080c-4454-9906-16f7389147c3"
      },
      "source": [
        "num_heads = 8\n",
        "d_model = 512\n",
        "dff = 2048 # Dimension of the feed-forward network inner layer\n",
        "dropout_rate = 0.1\n",
        "batch_size = 64\n",
        "seq_len = 100 # Example sequence length\n",
        "\n",
        "# Create an instance of EncoderLayer\n",
        "encoder_layer = EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "\n",
        "# Create dummy input tensor\n",
        "dummy_input_encoder = tf.random.uniform((batch_size, seq_len, d_model))\n",
        "\n",
        "# Call the encoder layer with dummy input (no mask, training=True for dropout effect)\n",
        "encoder_output = encoder_layer(dummy_input_encoder, training=True, mask=None)\n",
        "\n",
        "print(f\"EncoderLayer output shape: {encoder_output.shape}\") # Expected: (batch_size, seq_len, d_model)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EncoderLayer output shape: (64, 100, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11221912"
      },
      "source": [
        "## Construct Transformer Decoder Layer\n",
        "\n",
        "### Subtask:\n",
        "Implement a single transformer decoder layer, which includes masked multi-head self-attention, multi-head encoder-decoder attention, and a position-wise feed-forward network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "845075e9"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the first step to construct the Transformer Decoder Layer is to define the `DecoderLayer` class inheriting from `tf.keras.layers.Layer` and implement its `__init__` method, initializing all the necessary components such as two `MultiHeadAttention` layers, a `point_wise_feed_forward_network`, three `LayerNormalization` layers, and three `Dropout` layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b28507b4",
        "outputId": "f037799f-ce85-4b05-b59b-314a539611ba"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Masked self-attention\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Encoder-decoder attention\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "        print(f\"DecoderLayer initialized with d_model={d_model}, num_heads={num_heads}, dff={dff}, dropout_rate={rate}\")\n",
        "\n",
        "print(\"DecoderLayer class definition started with __init__.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoderLayer class definition started with __init__.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6630546"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to implement the `call` method within the `DecoderLayer` class, which will orchestrate the masked multi-head self-attention, multi-head encoder-decoder attention, feed-forward network, residual connections, layer normalization, and dropout.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d080d0d",
        "outputId": "8f08538b-25cd-469e-b347-90c31422829d"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Masked self-attention\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Encoder-decoder attention\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # Masked Multi-head self-attention\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # Multi-head encoder-decoder attention\n",
        "        # q = out1, k = enc_output, v = enc_output\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ffn_output = self.ffn(out2) # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "print(\"DecoderLayer class updated with call method.\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoderLayer class updated with call method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f96829d7"
      },
      "source": [
        "## Assemble Full Transformer Model\n",
        "\n",
        "### Subtask:\n",
        "Implement the Encoder part of the Transformer model by combining input embeddings, positional encoding, and multiple EncoderLayer instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05424123"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the first step is to define the `Encoder` class inheriting from `tf.keras.layers.Layer` and implement its `__init__` method, initializing the embedding layer, positional encoding layer, a list of encoder layers, and a dropout layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b120bad7",
        "outputId": "f9b65bdf-0b37-4ecf-c7ab-f36836dcd751"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "        print(f\"Encoder initialized with num_layers={num_layers}, d_model={d_model}, num_heads={num_heads}, dff={dff}, input_vocab_size={input_vocab_size}, maximum_position_encoding={maximum_position_encoding}, dropout_rate={rate}\")\n",
        "\n",
        "print(\"Encoder class definition started with __init__.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder class definition started with __init__.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8d415d5"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to implement the `call` method within the `Encoder` class, which will apply input embeddings, add positional encoding, apply dropout, and pass the data through multiple `EncoderLayer` instances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd468841",
        "outputId": "cc239722-3639-40bc-b37e-467189d5341a"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "print(\"Encoder class updated with call method.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder class updated with call method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7d8574a"
      },
      "source": [
        "## Assemble Full Transformer Model\n",
        "\n",
        "### Subtask:\n",
        "Implement the Decoder part of the Transformer model by combining input embeddings, positional encoding, and multiple DecoderLayer instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f793ee3"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to define the `Decoder` class, inheriting from `tf.keras.layers.Layer`, and implement its `__init__` method to initialize the embedding layer, positional encoding layer, multiple `DecoderLayer` instances, and a dropout layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b4c1307",
        "outputId": "7f960c7e-adcc-4ca7-861b-09c981fb2062"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                    look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "print(\"Decoder class definition completed with __init__ and call methods.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder class definition completed with __init__ and call methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "745b2b12"
      },
      "source": [
        "## Assemble Full Transformer Model\n",
        "\n",
        "### Subtask:\n",
        "Assemble the complete Transformer model by combining the encoder and decoder layers, along with input and output embeddings and linear transformations. This includes defining masking functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04987df4"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the first step to assemble the complete Transformer model is to define the `create_padding_mask` function, which generates a mask for padding tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9470dd5b",
        "outputId": "241fb743-11f0-4e5e-c5c0-4f1720519fc3"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, PAD_ID), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits. For example, for a sequence\n",
        "    # of length 3, the mask will be [[0, 0, 0], [0, 0, 0], [0, 0, 0]] if no padding\n",
        "    # or [[0, 0, 1], [0, 0, 1], [0, 0, 1]] if the last token is padding.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "print(\"create_padding_mask function defined.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create_padding_mask function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50493641"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to define the `create_look_ahead_mask` function, which creates a mask to hide future tokens in the decoder's self-attention mechanism.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1f1ec2a",
        "outputId": "de1c03bd-9ae5-42ce-d8d2-fc8d105bae99"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "print(\"create_look_ahead_mask function defined.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create_look_ahead_mask function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97f8a967"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to define the `Transformer` class, inheriting from `tf.keras.Model`. In its `__init__` method, I will initialize the `Encoder`, `Decoder`, and a final `tf.keras.layers.Dense` layer for the output vocabulary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "049cfcdc",
        "outputId": "ef609535-1f87-4a8c-fdc1-5d3f462af89c"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "        print(f\"Transformer initialized with num_layers={num_layers}, d_model={d_model}, num_heads={num_heads}, dff={dff}, input_vocab_size={input_vocab_size}, target_vocab_size={target_vocab_size}, pe_input={pe_input}, pe_target={pe_target}, dropout_rate={rate}\")\n",
        "\n",
        "print(\"Transformer class definition started with __init__.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer class definition started with __init__.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84e07c2b"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to implement the `call` method within the `Transformer` class. This method will orchestrate the data flow through the encoder and decoder, and then apply the final dense layer to produce the output logits.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0b33f92",
        "outputId": "5ae02750-2957-43cd-fa7b-f896dd9c8247"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights\n",
        "\n",
        "print(\"Transformer class updated with call method.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer class updated with call method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d83d470"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the final step for this subtask is to test the complete `Transformer` model by creating an instance with appropriate parameters and passing dummy input and target sequences to its `call` method. This will verify the output shape of the logits and the attention weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "9042a873",
        "outputId": "1e7a49d5-15c6-4378-fb42-b785f87856df"
      },
      "source": [
        "num_layers = 2\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 2048\n",
        "\n",
        "# Use the dummy vocab sizes and max sequence lengths from previous data preparation\n",
        "input_vocab_size = len(source_vocab)\n",
        "target_vocab_size = len(target_vocab)\n",
        "pe_input = max_input_seq_len + 10 # Add some buffer for positional encoding\n",
        "pe_target = max_target_seq_len + 10 # Add some buffer for positional encoding\n",
        "\n",
        "# Create an instance of the Transformer model\n",
        "transformer = Transformer(\n",
        "    num_layers,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    input_vocab_size,\n",
        "    target_vocab_size,\n",
        "    pe_input,\n",
        "    pe_target,\n",
        "    rate=0.1\n",
        ")\n",
        "\n",
        "# Prepare dummy input and target sequences (using previously created batches)\n",
        "# input_batch shape: (batch_size, inp_seq_len)\n",
        "# target_batch shape: (batch_size, tar_seq_len)\n",
        "dummy_input = input_batch # (3, 5)\n",
        "dummy_target = target_batch[:, :-1] # Decoder input needs to be shifted right (remove last token)\n",
        "\n",
        "# Generate masks\n",
        "enc_padding_mask = create_padding_mask(dummy_input)\n",
        "dec_padding_mask = create_padding_mask(dummy_input) # Used for encoder-decoder attention\n",
        "look_ahead_mask = create_look_ahead_mask(tf.shape(dummy_target)[1])\n",
        "combined_mask = tf.maximum(create_padding_mask(dummy_target), look_ahead_mask) # For decoder self-attention\n",
        "\n",
        "# Call the transformer model\n",
        "# Note: training=True to enable dropout\n",
        "final_output, attention_weights = transformer(\n",
        "    dummy_input,\n",
        "    dummy_target,\n",
        "    training=True,\n",
        "    enc_padding_mask=enc_padding_mask,\n",
        "    look_ahead_mask=combined_mask,\n",
        "    dec_padding_mask=dec_padding_mask\n",
        ")\n",
        "\n",
        "print(f\"Transformer output (logits) shape: {final_output.shape}\") # Expected: (batch_size, tar_seq_len, target_vocab_size)\n",
        "# Example of one attention weight shape from the decoder\n",
        "if 'decoder_layer1_block1' in attention_weights:\n",
        "    print(f\"Sample attention weights shape (decoder_layer1_block1): {attention_weights['decoder_layer1_block1'].shape}\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Transformer.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by Transformer.call():\n  • inp=tf.Tensor(shape=(3, 5), dtype=int32)\n  • tar=tf.Tensor(shape=(3, 6), dtype=int32)\n  • training=True\n  • enc_padding_mask=tf.Tensor(shape=(3, 1, 1, 5), dtype=float32)\n  • look_ahead_mask=tf.Tensor(shape=(3, 1, 6, 6), dtype=float32)\n  • dec_padding_mask=tf.Tensor(shape=(3, 1, 1, 5), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-179878066.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Call the transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Note: training=True to enable dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m final_output, attention_weights = transformer(\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdummy_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2939951223.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, inp_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Transformer.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by Transformer.call():\n  • inp=tf.Tensor(shape=(3, 5), dtype=int32)\n  • tar=tf.Tensor(shape=(3, 6), dtype=int32)\n  • training=True\n  • enc_padding_mask=tf.Tensor(shape=(3, 1, 1, 5), dtype=float32)\n  • look_ahead_mask=tf.Tensor(shape=(3, 1, 6, 6), dtype=float32)\n  • dec_padding_mask=tf.Tensor(shape=(3, 1, 1, 5), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32d914d6"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the `training` argument (a boolean) was passed as a positional argument to the `Encoder` and `Decoder` within the `Transformer`'s `call` method, but Keras layers expect such non-tensor arguments to be passed as keyword arguments. I will modify the `call` method in the `Transformer` class to pass `training` and `mask` arguments as keyword arguments to `self.encoder` and `self.decoder`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9065e8e9",
        "outputId": "abca9beb-34b4-4262-b4bf-578b7a6ae43f"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights\n",
        "\n",
        "print(\"Transformer class updated with call method to pass arguments as keywords.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer class updated with call method to pass arguments as keywords.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ab36463"
      },
      "source": [
        "**Reasoning**:\n",
        "After updating the `Transformer` class's `call` method to correctly pass `training` and mask arguments as keyword arguments, I need to re-test the complete `Transformer` model by creating an instance with appropriate parameters and passing dummy input and target sequences to its `call` method. This will verify the output shape of the logits and the attention weights and confirm that the previous error has been resolved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "a7d87593",
        "outputId": "d1313ab9-b34a-49ba-96ab-00b155960c78"
      },
      "source": [
        "num_layers = 2\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 2048\n",
        "\n",
        "# Use the dummy vocab sizes and max sequence lengths from previous data preparation\n",
        "input_vocab_size = len(source_vocab)\n",
        "target_vocab_size = len(target_vocab)\n",
        "pe_input = max_input_seq_len + 10 # Add some buffer for positional encoding\n",
        "pe_target = max_target_seq_len + 10 # Add some buffer for positional encoding\n",
        "\n",
        "# Create an instance of the Transformer model\n",
        "transformer = Transformer(\n",
        "    num_layers,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    input_vocab_size,\n",
        "    target_vocab_size,\n",
        "    pe_input,\n",
        "    pe_target,\n",
        "    rate=0.1\n",
        ")\n",
        "\n",
        "# Prepare dummy input and target sequences (using previously created batches)\n",
        "# input_batch shape: (batch_size, inp_seq_len)\n",
        "# target_batch shape: (batch_size, tar_seq_len)\n",
        "dummy_input = input_batch # (3, 5)\n",
        "dummy_target = target_batch[:, :-1] # Decoder input needs to be shifted right (remove last token)\n",
        "\n",
        "# Generate masks\n",
        "enc_padding_mask = create_padding_mask(dummy_input)\n",
        "dec_padding_mask = create_padding_mask(dummy_input) # Used for encoder-decoder attention\n",
        "look_ahead_mask = create_look_ahead_mask(tf.shape(dummy_target)[1])\n",
        "combined_mask = tf.maximum(create_padding_mask(dummy_target), look_ahead_mask) # For decoder self-attention\n",
        "\n",
        "# Call the transformer model\n",
        "# Note: training=True to enable dropout\n",
        "final_output, attention_weights = transformer(\n",
        "    dummy_input,\n",
        "    dummy_target,\n",
        "    training=True,\n",
        "    enc_padding_mask=enc_padding_mask,\n",
        "    look_ahead_mask=combined_mask,\n",
        "    dec_padding_mask=dec_padding_mask\n",
        ")\n",
        "\n",
        "print(f\"Transformer output (logits) shape: {final_output.shape}\") # Expected: (batch_size, tar_seq_len, target_vocab_size)\n",
        "# Example of one attention weight shape from the decoder\n",
        "if 'decoder_layer1_block1' in attention_weights:\n",
        "    print(f\"Sample attention weights shape (decoder_layer1_block1): {attention_weights['decoder_layer1_block1'].shape}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Exception encountered when calling Encoder.call().\n\n\u001b[1m'PositionalEncoding' object is not subscriptable\u001b[0m\n\nArguments received by Encoder.call():\n  • x=tf.Tensor(shape=(3, 5), dtype=int32)\n  • training=True\n  • mask=tf.Tensor(shape=(3, 1, 1, 5), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2290965427.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Call the transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Note: training=True to enable dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m final_output, attention_weights = transformer(\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdummy_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1810653826.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, inp_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4254313569.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling Encoder.call().\n\n\u001b[1m'PositionalEncoding' object is not subscriptable\u001b[0m\n\nArguments received by Encoder.call():\n  • x=tf.Tensor(shape=(3, 5), dtype=int32)\n  • training=True\n  • mask=tf.Tensor(shape=(3, 1, 1, 5), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6062845"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error 'PositionalEncoding' object is not subscriptable occurred because the `Encoder` and `Decoder` classes were attempting to subscript the `PositionalEncoding` layer object itself (`self.pos_encoding[:, :seq_len, :]`) instead of calling its `call` method (`self.pos_encoding(x)`). I will modify both the `Encoder` and `Decoder` classes' `call` methods to correctly apply the positional encoding layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2050db5f",
        "outputId": "47c2b32c-1cc6-4c0a-b13c-e59059de2f03"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        # x.shape == (batch_size, input_seq_len)\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Apply positional encoding by calling the layer\n",
        "        x = self.pos_encoding(x) # Corrected line\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "print(\"Encoder class updated with corrected positional encoding application.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder class updated with corrected positional encoding application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aeadc18"
      },
      "source": [
        "**Reasoning**:\n",
        "As stated in the previous reasoning, the `Decoder` class also needs to be modified to correctly apply the positional encoding layer by calling its `call` method, similar to the `Encoder` class fix. This ensures consistent and correct usage of the `PositionalEncoding` layer across the Transformer model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfe7e222",
        "outputId": "db125a8d-d617-48d6-d3e2-3e9fe09e08e5"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # x.shape == (batch_size, target_seq_len)\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Apply positional encoding by calling the layer\n",
        "        x = self.pos_encoding(x) # Corrected line\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                    look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "print(\"Decoder class updated with corrected positional encoding application.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder class updated with corrected positional encoding application.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22c31a44"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that both the `Encoder` and `Decoder` classes have been updated to correctly apply the positional encoding, I will re-test the complete `Transformer` model with dummy data and masks. This final test will verify that all previous errors have been resolved and that the model produces outputs with the expected shapes, thus completing the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "0553f01d",
        "outputId": "330202d7-9ec5-4672-ca48-fc466392d773"
      },
      "source": [
        "num_layers = 2\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 2048\n",
        "\n",
        "# Use the dummy vocab sizes and max sequence lengths from previous data preparation\n",
        "input_vocab_size = len(source_vocab)\n",
        "target_vocab_size = len(target_vocab)\n",
        "pe_input = max_input_seq_len + 10 # Add some buffer for positional encoding\n",
        "pe_target = max_target_seq_len + 10 # Add some buffer for positional encoding\n",
        "\n",
        "# Create an instance of the Transformer model\n",
        "transformer = Transformer(\n",
        "    num_layers,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    input_vocab_size,\n",
        "    target_vocab_size,\n",
        "    pe_input,\n",
        "    pe_target,\n",
        "    rate=0.1\n",
        ")\n",
        "\n",
        "# Prepare dummy input and target sequences (using previously created batches)\n",
        "# input_batch shape: (batch_size, inp_seq_len)\n",
        "# target_batch shape: (batch_size, tar_seq_len)\n",
        "dummy_input = input_batch # (3, 5)\n",
        "dummy_target = target_batch[:, :-1] # Decoder input needs to be shifted right (remove last token)\n",
        "\n",
        "# Generate masks\n",
        "enc_padding_mask = create_padding_mask(dummy_input)\n",
        "dec_padding_mask = create_padding_mask(dummy_input) # Used for encoder-decoder attention\n",
        "look_ahead_mask = create_look_ahead_mask(tf.shape(dummy_target)[1])\n",
        "combined_mask = tf.maximum(create_padding_mask(dummy_target), look_ahead_mask) # For decoder self-attention\n",
        "\n",
        "# Call the transformer model\n",
        "# Note: training=True to enable dropout\n",
        "final_output, attention_weights = transformer(\n",
        "    dummy_input,\n",
        "    dummy_target,\n",
        "    training=True,\n",
        "    enc_padding_mask=enc_padding_mask,\n",
        "    look_ahead_mask=combined_mask,\n",
        "    dec_padding_mask=dec_padding_mask\n",
        ")\n",
        "\n",
        "print(f\"Transformer output (logits) shape: {final_output.shape}\") # Expected: (batch_size, tar_seq_len, target_vocab_size)\n",
        "# Example of one attention weight shape from the decoder\n",
        "if 'decoder_layer1_block1' in attention_weights:\n",
        "    print(f\"Sample attention weights shape (decoder_layer1_block1): {attention_weights['decoder_layer1_block1'].shape}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Encoder.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by Encoder.call():\n  • x=tf.Tensor(shape=(3, 5), dtype=int32)\n  • training=True\n  • mask=tf.Tensor(shape=(3, 1, 1, 5), dtype=float32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2290965427.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Call the transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Note: training=True to enable dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m final_output, attention_weights = transformer(\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdummy_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1810653826.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, inp_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1554147739.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m  \u001b[0;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Encoder.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by Encoder.call():\n  • x=tf.Tensor(shape=(3, 5), dtype=int32)\n  • training=True\n  • mask=tf.Tensor(shape=(3, 1, 1, 5), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "062f2767"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous error in `Encoder.call()` stemmed from passing `training` and `mask` positionally to `EncoderLayer` within the loop. I need to modify the `Encoder` class's `call` method to ensure these arguments are passed as keyword arguments to each `EncoderLayer` instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "545b94f4",
        "outputId": "b8dd4ee0-cd8f-42fc-8dbb-66ac9066789f"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        # x.shape == (batch_size, input_seq_len)\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Apply positional encoding by calling the layer\n",
        "        x = self.pos_encoding(x) # Corrected line\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask) # Corrected line: pass as keyword arguments\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "print(\"Encoder class updated with corrected positional encoding and keyword argument passing.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder class updated with corrected positional encoding and keyword argument passing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b784e967"
      },
      "source": [
        "**Reasoning**:\n",
        "The `Encoder` class has been updated to correctly pass `training` and `mask` as keyword arguments to its internal `EncoderLayer` instances. However, the `Decoder` class also has a similar loop for `DecoderLayer` instances, and it's highly probable that it might encounter the same positional argument error if not fixed. Therefore, I need to update the `Decoder` class to ensure `training`, `look_ahead_mask`, and `padding_mask` are passed as keyword arguments to each `DecoderLayer` instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ad74869",
        "outputId": "431b0524-4ab4-47d8-9903-ee3fed4ab0d1"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # x.shape == (batch_size, target_seq_len)\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Apply positional encoding by calling the layer\n",
        "        x = self.pos_encoding(x) # Corrected line\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training,\n",
        "                                                    look_ahead_mask=look_ahead_mask, padding_mask=padding_mask) # Corrected line: pass as keyword arguments\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "print(\"Decoder class updated with corrected positional encoding and keyword argument passing.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder class updated with corrected positional encoding and keyword argument passing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c341b64"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that both the `Encoder` and `Decoder` classes have been updated to correctly pass the positional encoding, I will re-test the complete `Transformer` model with dummy data and masks. This final test will verify that all previous errors have been resolved and that the model produces outputs with the expected shapes, thus completing the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5abfa825",
        "outputId": "8e0de8df-edde-4d13-a6d4-ca3aac579370"
      },
      "source": [
        "num_layers = 2\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "dff = 2048\n",
        "\n",
        "# Use the dummy vocab sizes and max sequence lengths from previous data preparation\n",
        "input_vocab_size = len(source_vocab)\n",
        "target_vocab_size = len(target_vocab)\n",
        "pe_input = max_input_seq_len + 10 # Add some buffer for positional encoding\n",
        "pe_target = max_target_seq_len + 10 # Add some buffer for positional encoding\n",
        "\n",
        "# Create an instance of the Transformer model\n",
        "transformer = Transformer(\n",
        "    num_layers,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    input_vocab_size,\n",
        "    target_vocab_size,\n",
        "    pe_input,\n",
        "    pe_target,\n",
        "    rate=0.1\n",
        ")\n",
        "\n",
        "# Prepare dummy input and target sequences (using previously created batches)\n",
        "# input_batch shape: (batch_size, inp_seq_len)\n",
        "# target_batch shape: (batch_size, tar_seq_len)\n",
        "dummy_input = input_batch # (3, 5)\n",
        "dummy_target = target_batch[:, :-1] # Decoder input needs to be shifted right (remove last token)\n",
        "\n",
        "# Generate masks\n",
        "enc_padding_mask = create_padding_mask(dummy_input)\n",
        "dec_padding_mask = create_padding_mask(dummy_input) # Used for encoder-decoder attention\n",
        "look_ahead_mask = create_look_ahead_mask(tf.shape(dummy_target)[1])\n",
        "combined_mask = tf.maximum(create_padding_mask(dummy_target), look_ahead_mask) # For decoder self-attention\n",
        "\n",
        "# Call the transformer model\n",
        "# Note: training=True to enable dropout\n",
        "final_output, attention_weights = transformer(\n",
        "    dummy_input,\n",
        "    dummy_target,\n",
        "    training=True,\n",
        "    enc_padding_mask=enc_padding_mask,\n",
        "    look_ahead_mask=combined_mask,\n",
        "    dec_padding_mask=dec_padding_mask\n",
        ")\n",
        "\n",
        "print(f\"Transformer output (logits) shape: {final_output.shape}\") # Expected: (batch_size, tar_seq_len, target_vocab_size)\n",
        "# Example of one attention weight shape from the decoder\n",
        "if 'decoder_layer1_block1' in attention_weights:\n",
        "    print(f\"Sample attention weights shape (decoder_layer1_block1): {attention_weights['decoder_layer1_block1'].shape}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer output (logits) shape: (3, 6, 11)\n",
            "Sample attention weights shape (decoder_layer1_block1): (3, 8, 6, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e2cd085"
      },
      "source": [
        "## Define Custom Learning Rate Schedule and Optimizer\n",
        "\n",
        "### Subtask:\n",
        "Transformers often benefit from a specific learning rate schedule with a warm-up phase. Define this schedule and use an optimizer like Adam.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ec25459"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, I will define the `CustomSchedule` class, inheriting from `tf.keras.optimizers.schedules.LearningRateSchedule`, and implement its `__init__` and `__call__` methods to compute the learning rate with a warm-up phase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4818b82f",
        "outputId": "7190052d-444b-4b78-abf8-91d4cfbf15f7"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "print(\"CustomSchedule class defined.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomSchedule class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cab8e716"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, I will create an instance of the `CustomSchedule` and then initialize the `Adam` optimizer with this custom learning rate schedule and specified parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bcf1c55",
        "outputId": "a2184834-bfd3-4187-ac88-dbcaf39ca857"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model, warmup_steps=4000)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9\n",
        ")\n",
        "\n",
        "print(\"Custom learning rate schedule and Adam optimizer initialized.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom learning rate schedule and Adam optimizer initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55c399e9"
      },
      "source": [
        "## Train the Transformer Model\n",
        "\n",
        "### Subtask:\n",
        "Train the complete transformer model using the prepared dataset, custom learning rate schedule, and an appropriate loss function (e.g., SparseCategoricalCrossentropy for sequence-to-sequence tasks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3da2b5e"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the first step is to define the loss function using `tf.keras.losses.SparseCategoricalCrossentropy` with `from_logits=True` and define a custom accuracy metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b454d034",
        "outputId": "bbf0aa77-74f0-47a0-b2d4-cff67649609b"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    # Ensure we are not calculating loss for padding tokens\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, PAD_ID))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, PAD_ID))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "print(\"Loss function and accuracy metric defined.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function and accuracy metric defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "455b3a35"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the loss function and accuracy metric are defined, the next step is to compile the `Transformer` model with the custom optimizer, loss, and accuracy functions, and then train it using the prepared dummy data, ensuring correct masking and shifted target labels for the decoder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "284064d9",
        "outputId": "1e65b8bb-bfbe-4cbe-b1a4-95f7b86c26a7"
      },
      "source": [
        "epochs = 5 # You can adjust this for more training\n",
        "\n",
        "# Prepare the dataset for training\n",
        "# The decoder input 'tar' is target_batch[:, :-1]\n",
        "# The true labels 'tar_real' for loss calculation are target_batch[:, 1:]\n",
        "tar_inp = dummy_target # This is already target_batch[:, :-1]\n",
        "tar_real = target_batch[:, 1:] # Shifted target for calculating loss\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"Shape of encoder input (inp): {dummy_input.shape}\")\n",
        "print(f\"Shape of decoder input (tar_inp): {tar_inp.shape}\")\n",
        "print(f\"Shape of real target (tar_real): {tar_real.shape}\")\n",
        "\n",
        "\n",
        "# Training loop (simplified for demonstration, typically would use tf.data.Dataset)\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Generate masks for the current training step\n",
        "    enc_padding_mask = create_padding_mask(dummy_input)\n",
        "    dec_padding_mask = create_padding_mask(dummy_input) # Mask for encoder-decoder attention\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
        "    # Combined mask for decoder self-attention: padding mask + look ahead mask\n",
        "    combined_mask = tf.maximum(create_padding_mask(tar_inp), look_ahead_mask)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            dummy_input,\n",
        "            tar_inp,\n",
        "            training=True,\n",
        "            enc_padding_mask=enc_padding_mask,\n",
        "            look_ahead_mask=combined_mask,\n",
        "            dec_padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        current_loss = loss_function(tar_real, predictions)\n",
        "        current_accuracy = accuracy_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(current_loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    print(f\"Loss: {current_loss.numpy():.4f}, Accuracy: {current_accuracy.numpy():.4f}\")\n",
        "\n",
        "print(\"Transformer model training complete.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of encoder input (inp): (3, 5)\n",
            "Shape of decoder input (tar_inp): (3, 6)\n",
            "Shape of real target (tar_real): (3, 6)\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "cannot compute Equal as input #1(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:Equal] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1184904489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mcurrent_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2520562782.py\u001b[0m in \u001b[0;36maccuracy_function\u001b[0;34m(real, pred)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute Equal as input #1(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:Equal] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16d3ddc3"
      },
      "source": [
        "**Reasoning**:\n",
        "The `InvalidArgumentError` in the `accuracy_function` indicates a type mismatch between `real` (int32) and the output of `tf.argmax(pred, axis=2)` (int64). I need to cast the output of `tf.argmax` to `tf.int32` to resolve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0020e4d",
        "outputId": "4c14540b-3da1-490d-bb51-f1a5b09e2899"
      },
      "source": [
        "epochs = 5 # You can adjust this for more training\n",
        "\n",
        "# Prepare the dataset for training\n",
        "# The decoder input 'tar' is target_batch[:, :-1]\n",
        "# The true labels 'tar_real' for loss calculation are target_batch[:, 1:]\n",
        "tar_inp = dummy_target # This is already target_batch[:, :-1]\n",
        "tar_real = target_batch[:, 1:] # Shifted target for calculating loss\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"Shape of encoder input (inp): {dummy_input.shape}\")\n",
        "print(f\"Shape of decoder input (tar_inp): {tar_inp.shape}\")\n",
        "print(f\"Shape of real target (tar_real): {tar_real.shape}\")\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    # Cast the argmax output to int32 to match 'real' tensor's dtype\n",
        "    accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), tf.int32))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, PAD_ID))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "# Training loop (simplified for demonstration, typically would use tf.data.Dataset)\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Generate masks for the current training step\n",
        "    enc_padding_mask = create_padding_mask(dummy_input)\n",
        "    dec_padding_mask = create_padding_mask(dummy_input) # Mask for encoder-decoder attention\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
        "    # Combined mask for decoder self-attention: padding mask + look ahead mask\n",
        "    combined_mask = tf.maximum(create_padding_mask(tar_inp), look_ahead_mask)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            dummy_input,\n",
        "            tar_inp,\n",
        "            training=True,\n",
        "            enc_padding_mask=enc_padding_mask,\n",
        "            look_ahead_mask=combined_mask,\n",
        "            dec_padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        current_loss = loss_function(tar_real, predictions)\n",
        "        current_accuracy = accuracy_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(current_loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    print(f\"Loss: {current_loss.numpy():.4f}, Accuracy: {current_accuracy.numpy():.4f}\")\n",
        "\n",
        "print(\"Transformer model training complete.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of encoder input (inp): (3, 5)\n",
            "Shape of decoder input (tar_inp): (3, 6)\n",
            "Shape of real target (tar_real): (3, 6)\n",
            "\n",
            "Epoch 1/5\n",
            "Loss: 3.3204, Accuracy: 0.1333\n",
            "\n",
            "Epoch 2/5\n",
            "Loss: 3.2240, Accuracy: 0.1333\n",
            "\n",
            "Epoch 3/5\n",
            "Loss: 3.0856, Accuracy: 0.2000\n",
            "\n",
            "Epoch 4/5\n",
            "Loss: 3.1914, Accuracy: 0.1333\n",
            "\n",
            "Epoch 5/5\n",
            "Loss: 3.1485, Accuracy: 0.1333\n",
            "Transformer model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada23d80"
      },
      "source": [
        "## Evaluate the Transformer Model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained transformer model on a test dataset, possibly using metrics like BLEU score for machine translation tasks, or accuracy for other sequence tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2209a677"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the first step is to set up a function for making predictions with the Transformer model. This function will iteratively generate tokens, handling the necessary masking and utilizing the previously defined `transformer` model components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e45c735",
        "outputId": "26946cd5-56eb-445f-ee7f-626f5e6c722a"
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "    # The decoder input is initially just the start token\n",
        "    decoder_input = tf.expand_dims([target_id_to_word['<start>']], 0)\n",
        "\n",
        "    output = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "    # Assuming max_target_seq_len is defined and accessible\n",
        "    # If not, we would need to pass it or infer it\n",
        "    for i in range(max_target_seq_len): # Iterate up to max_target_seq_len\n",
        "        enc_padding_mask = create_padding_mask(encoder_input)\n",
        "        dec_padding_mask = create_padding_mask(encoder_input) # Mask for encoder-decoder attention\n",
        "\n",
        "        look_ahead_mask = create_look_ahead_mask(tf.shape(decoder_input)[1])\n",
        "        # Combined mask for decoder self-attention: padding mask + look ahead mask\n",
        "        combined_mask = tf.maximum(create_padding_mask(decoder_input), look_ahead_mask)\n",
        "\n",
        "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            decoder_input,\n",
        "            training=False, # Set to False for evaluation mode\n",
        "            enc_padding_mask=enc_padding_mask,\n",
        "            look_ahead_mask=combined_mask,\n",
        "            dec_padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        # Select the last token from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # Return the prediction if it's the end token\n",
        "        if tf.equal(predicted_id, target_id_to_word['<end>']):\n",
        "            break\n",
        "\n",
        "        # Concatentate the predicted_id to the output which is given to the decoder\n",
        "        # as its input.\n",
        "        decoder_input = tf.concat([decoder_input, predicted_id], axis=-1)\n",
        "        output = output.write(i, predicted_id[0, 0])\n",
        "\n",
        "    return tf.expand_dims(output.stack(), 0), attention_weights\n",
        "\n",
        "print(\"Prediction function 'evaluate' defined.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction function 'evaluate' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc0e18f9"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, the next step is to apply the `evaluate` function to the dummy input sequences to get predictions, then compare these predictions with the actual target sequences using the `accuracy_function`, and finally print the resulting accuracy to assess the model's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61ce23e4",
        "outputId": "20607901-c226-4d4f-c64a-2c2b526ef7f6"
      },
      "source": [
        "predicted_sequences = []\n",
        "actual_target_sequences = []\n",
        "\n",
        "# Iterate through each sample in the dummy_input batch\n",
        "for i in range(tf.shape(dummy_input)[0]):\n",
        "    inp_sample = dummy_input[i]\n",
        "    actual_target_sample = tar_real[i] # Get the corresponding actual target sequence\n",
        "\n",
        "    # Generate prediction for the current input sample\n",
        "    result, _ = evaluate(inp_sample)\n",
        "\n",
        "    # result.shape is (1, sequence_length), remove the batch dimension\n",
        "    predicted_sequences.append(tf.squeeze(result))\n",
        "    actual_target_sequences.append(actual_target_sample)\n",
        "\n",
        "# Pad predicted sequences to match the length of tar_real for accuracy calculation\n",
        "# The padding token should be PAD_ID (0)\n",
        "\n",
        "# Find the maximum length among predicted sequences for padding\n",
        "max_predicted_len = max(tf.shape(s)[0] for s in predicted_sequences)\n",
        "\n",
        "# Pad each predicted sequence\n",
        "final_predicted_sequences = []\n",
        "for seq in predicted_sequences:\n",
        "    current_len = tf.shape(seq)[0]\n",
        "    padding_needed = max_predicted_len - current_len\n",
        "    if padding_needed > 0:\n",
        "        # Pad with PAD_ID (0) at the end\n",
        "        padded_seq = tf.pad(seq, [[0, padding_needed]], \"CONSTANT\", constant_values=PAD_ID)\n",
        "    else:\n",
        "        padded_seq = seq\n",
        "    final_predicted_sequences.append(padded_seq)\n",
        "\n",
        "# Convert lists of tensors to a single tensor for batch processing\n",
        "# Need to ensure all sequences have the same length after padding for stacking\n",
        "final_predicted_sequences_tensor = tf.stack(final_predicted_sequences)\n",
        "actual_target_sequences_tensor = tf.stack(actual_target_sequences)\n",
        "\n",
        "# Ensure the lengths match for accuracy calculation by potentially truncating or further padding\n",
        "# If max_predicted_len > actual_target_sequences_tensor.shape[1], we might need to truncate predictions\n",
        "# If max_predicted_len < actual_target_sequences_tensor.shape[1], we need to pad predictions or handle mismatch\n",
        "\n",
        "# For simplicity, let's assume tar_real's length is the reference and truncate/pad predictions\n",
        "reference_len = tf.shape(tar_real)[1]\n",
        "\n",
        "# Adjust final_predicted_sequences_tensor to match reference_len\n",
        "if tf.shape(final_predicted_sequences_tensor)[1] > reference_len:\n",
        "    final_predicted_sequences_tensor = final_predicted_sequences_tensor[:, :reference_len]\n",
        "elif tf.shape(final_predicted_sequences_tensor)[1] < reference_len:\n",
        "    padding_needed_final = reference_len - tf.shape(final_predicted_sequences_tensor)[1]\n",
        "    final_predicted_sequences_tensor = tf.pad(final_predicted_sequences_tensor, [[0, 0], [0, padding_needed_final]], \"CONSTANT\", constant_values=PAD_ID)\n",
        "\n",
        "\n",
        "# Reshape predictions to match the (batch_size, seq_len, vocab_size) format expected by accuracy_function\n",
        "# We'll create a one-hot like representation for the predicted IDs for accuracy_function\n",
        "\n",
        "batch_size = tf.shape(final_predicted_sequences_tensor)[0]\n",
        "seq_len_pred = tf.shape(final_predicted_sequences_tensor)[1]\n",
        "vocab_size = target_vocab_size\n",
        "\n",
        "# Create a dummy one-hot tensor where the predicted ID has a high logit\n",
        "# This is a simplification; in a real scenario, you'd use the model's logits\n",
        "# For this demonstration, we are just comparing predicted_id with real_id.\n",
        "# The accuracy_function actually expects logits for `pred` argument.\n",
        "# So, we need to create a tensor that has the correct shape and can be argmaxed.\n",
        "\n",
        "# Create a tensor of zeros with shape (batch_size, seq_len_pred, vocab_size)\n",
        "# Set the value at the predicted index to 1 (or any high value) for argmax to pick it.\n",
        "predicted_logits_for_accuracy = tf.one_hot(final_predicted_sequences_tensor, depth=vocab_size, dtype=tf.float32) * 100.0\n",
        "\n",
        "# Calculate accuracy using the defined accuracy_function\n",
        "# Use tar_real as the 'real' values for comparison\n",
        "overall_accuracy = accuracy_function(tar_real, predicted_logits_for_accuracy)\n",
        "\n",
        "print(f\"Predicted sequence shape (after padding/truncation): {final_predicted_sequences_tensor.shape}\")\n",
        "print(f\"Actual target sequence shape: {tar_real.shape}\")\n",
        "print(f\"Overall Evaluation Accuracy: {overall_accuracy.numpy():.4f}\")\n",
        "\n",
        "# Optionally, print a sample prediction for verification\n",
        "print(\"\\nSample Prediction:\")\n",
        "for i in range(min(len(input_sequences_raw), 1)): # Print the first sample\n",
        "    input_decoded = decode_sequence(input_batch[i].numpy(), source_vocab)\n",
        "    actual_decoded = decode_sequence(tar_real[i].numpy(), target_vocab)\n",
        "    predicted_decoded = decode_sequence(final_predicted_sequences_tensor[i].numpy(), target_vocab)\n",
        "\n",
        "    print(f\"Input: {input_decoded}\")\n",
        "    print(f\"Actual: {actual_decoded}\")\n",
        "    print(f\"Predicted: {predicted_decoded}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sequence shape (after padding/truncation): (3, 6)\n",
            "Actual target sequence shape: (3, 6)\n",
            "Overall Evaluation Accuracy: 0.1333\n",
            "\n",
            "Sample Prediction:\n",
            "Input: hello world how are you\n",
            "Actual: bonjour monde comment allez vous <end>\n",
            "Predicted: allez allez allez allez allez allez\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "828e5f88"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the implementation process and the performance of the transformer model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8171177"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The task was to summarize the implementation process and the performance of the transformer model.\n",
        "\n",
        "*   **Implementation Process**: The implementation followed a modular approach, starting with an understanding of Transformer architecture, then building individual components such as Positional Encoding, Multi-Head Attention, Encoder Layer, and Decoder Layer. These components were then assembled into a full Transformer model. Custom masks, a learning rate schedule, and an optimizer were defined. Finally, a training loop and an evaluation function were implemented. The process involved several debugging steps, particularly during the full model assembly and training, to resolve issues related to Keras layer argument passing and data type mismatches.\n",
        "*   **Performance of the Transformer Model**: The model's performance on the dummy dataset was very poor, achieving an overall evaluation accuracy of approximately $0.1333$. Qualitative observation showed that the model generated highly repetitive predictions (e.g., \"allez allez allez allez allez allez\") instead of meaningful translations.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Transformer Architecture**: A comprehensive understanding of key components like self-attention, multi-head attention, positional encoding, and the encoder-decoder structure was established.\n",
        "*   **Positional Encoding**: The `PositionalEncoding` class was successfully implemented, generating sinusoidal encodings of shape `(1, 50, 512)` for a `max_sequence_length` of 50 and `d_model` of 512.\n",
        "*   **Data Preparation**: Dummy source and target vocabularies were defined, and raw numericalized sequences were created. These were then padded to `(3, 5)` for input and `(3, 7)` for target, and batched into TensorFlow tensors, including `<start>`, `<end>`, and `<pad>` tokens.\n",
        "*   **Multi-Head Attention**: A `MultiHeadAttention` layer and the `scaled_dot_product_attention` function were implemented. Testing with dummy inputs (`batch_size=64`, `seq_len=100`, `d_model=512`, `num_heads=8`) resulted in an output shape of `(64, 100, 512)` and attention weights of `(64, 8, 100, 100)`, confirming correct functionality.\n",
        "*   **Encoder and Decoder Layers**: A single `EncoderLayer` and `DecoderLayer` were successfully implemented, incorporating multi-head attention, feed-forward networks, residual connections, layer normalization, and dropout.\n",
        "*   **Full Transformer Model Assembly**: The complete `Transformer` model was assembled, integrating the `Encoder` and `Decoder` components. This involved defining `create_padding_mask` and `create_look_ahead_mask` functions. During assembly and testing, several critical errors were identified and resolved, including:\n",
        "    *   Incorrect positional argument passing to Keras layers within `Transformer.call`.\n",
        "    *   Incorrect application of `PositionalEncoding` (subscripting instead of calling the layer) within `Encoder.call` and `Decoder.call`.\n",
        "    *   Incorrect positional argument passing to `EncoderLayer` and `DecoderLayer` instances.\n",
        "    After resolution, the model produced correct output shapes: logits `(batch_size, tar_seq_len, target_vocab_size)` and attention weights `(batch_size, num_heads, tar_seq_len, tar_seq_len)`.\n",
        "*   **Learning Rate Schedule and Optimizer**: A `CustomSchedule` learning rate policy was implemented, featuring a warm-up phase, and used to initialize an `Adam` optimizer with `beta_1=0.9`, `beta_2=0.98`, and `epsilon=1e-9`.\n",
        "*   **Training Process**: A training loop was implemented with `tf.keras.losses.SparseCategoricalCrossentropy` and a custom `accuracy_function` that masked padding tokens. During initial training, a `TypeError` was encountered in `accuracy_function` due to a mismatch between `real` (int32) and `tf.argmax(pred, axis=2)` (int64), which was resolved by casting `tf.argmax` output to `tf.int32`. The model was trained for 5 epochs on the dummy data.\n",
        "*   **Evaluation**: An `evaluate` function was defined for inference. The model was evaluated on the dummy test set, resulting in a low overall accuracy of approximately $0.1333$. Qualitative analysis of sample predictions revealed repetitive output tokens.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The current model performance is poor, likely due to training on a very small and simple dummy dataset with only 3 samples for 5 epochs. To achieve meaningful results, the model requires training on a much larger and more diverse dataset relevant to a specific NLP task (e.g., machine translation with millions of sentence pairs) and for significantly more epochs.\n",
        "*   Further hyperparameter tuning (e.g., `d_model`, `num_heads`, `dff`, `dropout_rate`, `warmup_steps`, number of layers) and optimization techniques (e.g., gradient clipping, label smoothing) would be essential to improve model performance on a real-world task.\n"
      ]
    }
  ]
}